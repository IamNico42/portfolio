## **1.2 Lineare Regression**

### 🎯 **Ziel:**

Ein Modell, das eine lineare Beziehung zwischen einer Eingabe XXX und einer Ausgabe yyy findet.

### 📝 **Mathematisches Modell:**

$$y=wX+b$$

- X ist die Eingabematrix mit m Beispielen und n Features.
- w ist der Vektor der Gewichte.
- b ist der Bias-Wert.

### 🔢 **Fehlermetrik: Mean Squared Error (MSE)**

$$J(w,b)=m1​i=1∑m​(yi​−(yi​^​))^2 $$
$$=\frac{1}{m}∑m​(yi​−(wxi​+b))^2$$

### 🔽 **Gradientenabstieg zur Optimierung**

$$w := w - \alpha \frac{\partial J}{\partial w}$$
$$b := b - \alpha \frac{\partial J}{\partial b}$$

### 🖥️ **Python-Implementierung**

```python
import numpy as np

  

# Daten generieren
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# Parameter initialisieren
w = 0.000
b = 0.000
alpha = 0.0025  # Lernrate
epochs = 1000

# Listen zum Speichern von Änderungen
w_history = []
b_history = []

# Gradientenabstieg
for _ in range(epochs):
    y_pred = w * X.flatten() + b  # X muss flach sein
    dw = (-2 / len(X)) * np.sum(X.flatten() * (y - y_pred))  # Skalar
    db = (-2 / len(X)) * np.sum(y - y_pred)  # Skalar
    w -= alpha * dw
    b -= alpha * db
    w_history.append(w)
    b_history.append(b)

print(f"Trainiertes Modell: y = {w}x + {b}")

import matplotlib.pyplot as plt

plt.scatter(X, y, label="Daten")
plt.plot(X, w*X + b, color='red', label="Regressionslinie")
plt.legend()
plt.show()
```

![[1_iB5TFe77kfkWIxzcLT92Wg.gif]]